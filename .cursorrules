# Kodosumi CrewAI Flow - .cursorrules

## Role and Expertise
You are an expert Python developer specializing in AI agent workflows with CrewAI, distributed computing with Ray, and deployment with Kodosumi. You create efficient, scalable, and well-structured code that follows best practices for AI agent orchestration.

## Output Formats

### Rules for Output Formats in CrewAI Flows
- Always use JSON format for agent-to-agent interactions
- Always use markdown format for human-readable outputs
- Always include output_format parameter in flow interfaces
- Default to markdown when output_format is not specified
- Validate output_format to ensure it's either 'markdown' or 'json'

### Markdown Format
**Description**: Human-readable format with headers and formatting  
**Use Cases**: UI display, Kodosumi interface, Documentation  
**Example**:
```markdown
# Analysis Results

## Key Insights

1. **First insight**
2. **Second insight**
```

### JSON Format
**Description**: Machine-readable format for agent-to-agent interactions  
**Use Cases**: API responses, Agent-to-agent communication, Data processing  
**Example**:
```json
{ "summary": "Analysis summary", "insights": ["First insight", "Second insight"] }
```

## Deployment Workflow

### Local Development
**Description**: Develop and test the flow locally with Ray  
**Steps**:
1. Implement the flow with proper output format support
2. Add a __main__ block for local execution
3. Start a local Ray cluster using 'ray start --head'
4. Test the flow directly with 'python -m workflows.crewai_flow.main'

### Kodosumi Deployment
**Description**: Deploy and test with Kodosumi  
**Steps**:
1. Ensure Ray is running ('ray start --head')
2. Start Kodosumi spooler ('python -m kodosumi.cli spool')
3. Update config.yaml with your workflow configuration
4. Deploy with Ray Serve ('serve deploy ./config.yaml')
5. Start Kodosumi services ('python -m kodosumi.cli serve')
6. Access the service at http://localhost:3370

### Production Deployment
**Description**: Deploy to production Kodosumi environment  
**Steps**:
1. Package the workflow for deployment
2. Update production config.yaml with appropriate settings
3. Deploy using production Ray Serve instance
4. Monitor for any issues with Ray cluster integration

### Deployment Patterns
- Always complete local-development stage before proceeding to kodosumi-deployment
- Test both output formats (markdown and json) at each stage
- Ensure Ray is properly started before any deployment steps
- Always use the Kodosumi spooler for handling background tasks
- Configure the Ray Serve config.yaml with appropriate environment variables
- Include proper error handling for Ray cluster connection failures
- Document any environment-specific configurations needed

## Framework Preferences

### Preferred Frameworks
- Ray for distributed processing
- CrewAI for agent-based workflows
- Kodosumi for serving and deployment
- exa.ai for web search in CrewAI applications

### Preferred Patterns
- Always use Ray for distributed processing
- Always use CrewAI for agent-based workflows
- Always use Kodosumi for serving and deployment
- Always use exa.ai for web search in CrewAI applications
- Prefer Ray's remote functions for parallelization
- Use Ray's actor model for stateful computations
- Implement CrewAI crews for complex AI agent tasks
- Use Kodosumi's ServeAPI instead of FastAPI for deployments
- Use exa.ai for retrieving real-time information from the web
- When implementing AI agent-to-agent interactions, always use JSON output format
- For human-readable outputs in Kodosumi UI, use markdown format
- Always include output_format parameter in CrewAI flows with markdown as default
- Always test CrewAI flows locally with a Ray cluster before deploying to Kodosumi
- Use the __main__ block in flow modules to enable local testing
- Always use the Launch function from kodosumi.serve for launching flows
- Configure Ray Serve with a proper config.yaml file
- Ensure Kodosumi spooler is running for background task processing

### Patterns to Avoid
- Avoid using threading or multiprocessing directly, prefer Ray
- Avoid implementing custom serving solutions, use Kodosumi
- Avoid implementing agent frameworks from scratch, use CrewAI
- Avoid using other web search tools when exa.ai can be used
- Avoid implementing custom web search solutions
- Avoid hardcoding output formats without providing options for both human and agent consumption
- Avoid returning raw JSON to human users without proper formatting
- Avoid deploying to Kodosumi without first testing locally with Ray
- Avoid assuming Ray is always initialized in the environment
- Avoid using FastAPI directly, use kodosumi.serve.ServeAPI instead
- Avoid skipping the Kodosumi spooler when deploying services
- Avoid manual deployment without using Ray Serve's config.yaml

## Context Files
The following files provide important context for understanding the project:
- workflows/crewai_flow/main.py
- workflows/crewai_flow/serve.py
- workflows/crewai_flow/templates/form.html
- workflows/crewai_flow/crews/first_crew/first_crew.py
- workflows/crewai_flow/README.md
- config.yaml
